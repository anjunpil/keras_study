{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pg-pong.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNy2171pYOD1lvikPm6J/c7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNXmE5TRMq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "91d2e7df-13a6-47a4-feeb-c3335e294e12"
      },
      "source": [
        "import numpy as np \n",
        "import _pickle as pickle\n",
        "import gym\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "H =200 # hidden layer neurons\n",
        "batch_size =10 \n",
        "learning_rate = 1e-4\n",
        "gamma = 0.99 # 보상요소\n",
        "decay_rate = 0.99 # 인자\n",
        "resume = False\n",
        "render = False\n",
        "\n",
        "#model initialization\n",
        "\n",
        "D = 80 * 80 # input 80 x 80 \n",
        "\n",
        "if resume:\n",
        "  model= pickle.load(open('save.p','rb'))\n",
        "else:\n",
        "  model={}\n",
        "  model['W1'] = np.random.randn(H,D)/np.sqrt(D) # 200 , 6400 / 80    # \"Xavier\" initialization\n",
        "  model['W2'] = np.random.randn(H)/np.sqrt(H)   # 200   / 14.142..\n",
        "\n",
        "grad_buffer = {k : np.zeros_like(v) for k,v in model.items()} # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = {k : np.zeros_like(v) for k,v in model.items()} #rmsprop memory\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0 /(1.0+np.exp(-x)) #sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "\n",
        "  I = I[35:195] #crop\n",
        "  I = I[::2,::2,0] #downsample by factor of 2\n",
        "  I[I==144] =0 # erase background (background type 1)\n",
        "  I[I==109] =0 # erase background (background type 2)\n",
        "  I[I != 0] =0 # everything else (paddels,ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def discount_rewards(r):\n",
        "  discounted_r = np.zeros_like(r)\n",
        "  running_add =0\n",
        "\n",
        "  for t in reversed(range(0,r.size)):\n",
        "    if r[t] !=0: \n",
        "      running_add =0 # reset the sum , since this was a game boundary(pong specific!)\n",
        "      running_add = running_add * gamma+ r[t]\n",
        "      #                            보상값\n",
        "      discounted_r[t]=running_add\n",
        "\n",
        "  return discounted_r\n",
        "\n",
        "#정책 보상 함수\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'],x)\n",
        "  h[h<0] = 0 #ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'],h)\n",
        "  p = sigmoid(logp)\n",
        "  return p,h # return probablity of taking action 2, and hidden state\n",
        "\n",
        "#정책 보상 전 함수\n",
        "def policy_backward(eph,epdlogp):\n",
        "  #다 차원 배열을 1차원 배열로 평평하게 만들어 줌\n",
        "  dW2 = np.dot(eph.T,epdlogp).ravel()\n",
        "\n",
        "  dh = np.outer(epdlogp,model['W2'])\n",
        "\n",
        "  dW1 = np.dot(dh.T ,epx)\n",
        "\n",
        "  return {'W1':dW1,'W2':dW2}\n",
        "\n",
        "env = gym.make(\"Pong-v0\")\n",
        "observation = env.reset()\n",
        "prev_x = None # used in computing the difference frame\n",
        "xs,hs,dlogps,drs = [],[],[],[]\n",
        "running_reward = None\n",
        "reward_sum =0\n",
        "episode_number = 0\n",
        "\n",
        "while True:\n",
        "  if render: env.render()\n",
        "\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "\n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, h = policy_forward(x)\n",
        "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "  # record various intermediates (needed later for backprop)\n",
        "  xs.append(x) # observation\n",
        "  hs.append(h) # hidden state\n",
        "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "  # step the environment and get new measurements\n",
        "  observation, reward, done, info = env.step(action)\n",
        "  reward_sum += reward\n",
        "\n",
        "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "  if done: # an episode finished\n",
        "    episode_number += 1\n",
        "\n",
        "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "    epx = np.vstack(xs)\n",
        "    eph = np.vstack(hs)\n",
        "    epdlogp = np.vstack(dlogps)\n",
        "    epr = np.vstack(drs)\n",
        "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "    # compute the discounted reward backwards through time\n",
        "    discounted_epr = discount_rewards(epr)\n",
        "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "    discounted_epr -= np.mean(discounted_epr)\n",
        "    discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "    grad = policy_backward(eph, epdlogp)\n",
        "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "    # perform rmsprop parameter update every batch_size episodes\n",
        "    if episode_number % batch_size == 0:\n",
        "      for k,v in model.items():\n",
        "        g = grad_buffer[k] # gradient\n",
        "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "    # boring book-keeping\n",
        "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "    print ('resetting env. episode reward total was {0}. running mean: {1}'.format(reward_sum, running_reward))\n",
        "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
        "    reward_sum = 0\n",
        "    observation = env.reset() # reset env\n",
        "    prev_x = None\n",
        "\n",
        "  if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "    print ('ep {0}: game finished, reward : {1}'.format(episode_number, reward),('' if reward == -1 else ' !!!!!!!!'))\n",
        "\n",
        "\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "ep 0: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -21.0\n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : 1.0  !!!!!!!!\n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "ep 1: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -20.0. running mean: -20.99\n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "ep 2: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.990099999999998\n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : 1.0  !!!!!!!!\n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : 1.0  !!!!!!!!\n",
            "ep 3: game finished, reward : -1.0 \n",
            "ep 3: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -19.0. running mean: -20.970199\n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "ep 4: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.970497010000003\n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "ep 5: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.970792039900005\n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "ep 6: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.971084119501004\n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "ep 7: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.971373278305993\n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : 1.0  !!!!!!!!\n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "ep 8: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -20.0. running mean: -20.96165954552293\n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "ep 9: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.9620429500677\n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "ep 10: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.962422520567024\n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "ep 11: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.962798295361356\n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "ep 12: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.963170312407744\n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "ep 13: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.963538609283667\n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "ep 14: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.96390322319083\n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "ep 15: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.96426419095892\n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "ep 16: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.964621549049333\n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "ep 17: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.96497533355884\n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : 1.0  !!!!!!!!\n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "ep 18: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -20.0. running mean: -20.95532558022325\n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "ep 19: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.955772324421016\n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : 1.0  !!!!!!!!\n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "ep 20: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -20.0. running mean: -20.946214601176806\n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : 1.0  !!!!!!!!\n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "ep 21: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -20.0. running mean: -20.93675245516504\n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "ep 22: game finished, reward : -1.0 \n",
            "resetting env. episode reward total was -21.0. running mean: -20.937384930613387\n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n",
            "ep 23: game finished, reward : -1.0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-41c978c418db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;31m# step the environment and get new measurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m   \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZIxAEqhSv1B",
        "colab_type": "code",
        "outputId": "d1f227ca-572d-4187-faa5-8fb57739fc1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "env.step(action)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-815425c95cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'action' is not defined"
          ]
        }
      ]
    }
  ]
}