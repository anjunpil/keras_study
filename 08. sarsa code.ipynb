{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from environment_sarsa import Env\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    env = Env()\\n    agent = SARSAgent(actions=list(range(env.n_actions)))\\n    \\n    for episode in range(1000):\\n        state = env.reset()\\n        \\n        action = agent.get_action(state)\\n        \\n        while True:\\n            env.render()\\n            \\n            #행동 후 다음상태 보상 episode 종료 여부\\n            next_state,reward,done =env.step(action)\\n            \\n            next_action = agent.get_action(next_state)\\n            \\n            agent.learn(state,action,reward,next_state,next_action)\\n            \\n            state = next_state\\n            action =next_action\\n            \\n            #q함수 모두 표시\\n            env.print_value_all(agent.q_table)\\n            \\n            if done:\\n                break'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SARSAgent:\n",
    "    def __init__(self,actions):\n",
    "        self.actions =actions\n",
    "        self.step_size =0.01\n",
    "        self.discount_factor=0.9\n",
    "        self.epsilon =0.1\n",
    "        #0을 초기값을 가지는 큐함수 테이블 생성\n",
    "        self.q_table = defaultdict(lambda: [0.0,0.0,0.0,0.0])\n",
    "    \n",
    "    #s,a,r_t+1,s_t+1,a_t+1\n",
    "    def learn(self,state,action,reward,next_state,next_action):\n",
    "        state,next_state = str(state) ,str(next_state)\n",
    "        current_q = self.q_table[state][action]\n",
    "        next_state_q = self.q_table[next_state][next_action]\n",
    "        #td = R_t+1 + γ*Q(S_t+1,A_t+1) - Q(S_t,A_t)\n",
    "        td= reward +self.discount_factor * next_state_q - current_q\n",
    "        # Q(S_t,A_t)= Q(S_t,A_t) +a(R_t+1 + γ*Q(S_t+1,A_t+1) - Q(S_t,A_t))\n",
    "        new_q= current_q +self.step_size*td\n",
    "        self.q_table[state][action] =new_q\n",
    "        \n",
    "    #입실론 탐욕 정책에 따라 행동 반환\n",
    "    def get_action(self,state):\n",
    "        #epsilon 값 보다 작을 시 새로운 탐험을 하겠다\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        #나머지는 Q함수에 따른 행동을 하겠다\n",
    "        else:\n",
    "            state = str(state)\n",
    "            q_list =self.q_table[state]\n",
    "            action =arg_max(q_list)\n",
    "        \n",
    "        #그리고 그 행동을 하겠다\n",
    "        return action\n",
    "    \n",
    "    def arg_max(q_list):\n",
    "        #q_list가 가장 큰 값 true 반환\n",
    "        max_idx_list = np.argwhere(q_list == np.amax(q_list))\n",
    "        #flatten 이중 list 단일로\n",
    "        max_idx_list = max_idx_list.flatten().tolist()\n",
    "        return random.choice(max_idx_list)\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = SARSAgent(actions=list(range(env.n_actions)))\n",
    "    \n",
    "    for episode in range(1000):\n",
    "        state = env.reset()\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        while True:\n",
    "            env.render()\n",
    "            \n",
    "            #행동 후 다음상태 보상 episode 종료 여부\n",
    "            next_state,reward,done =env.step(action)\n",
    "            \n",
    "            next_action = agent.get_action(next_state)\n",
    "            \n",
    "            agent.learn(state,action,reward,next_state,next_action)\n",
    "            \n",
    "            state = next_state\n",
    "            action =next_action\n",
    "            \n",
    "            #q함수 모두 표시\n",
    "            env.print_value_all(agent.q_table)\n",
    "            \n",
    "            if done:\n",
    "                break'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
