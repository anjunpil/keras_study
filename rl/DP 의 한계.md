- 계산복잡도
  - 계산복잡도는 상태 크기의 3제곱에 비례
- 차원의 저주
  - 상태의 차원이 늘어나면 상태의 수가 지수적으로 증가
- 환경에 대한 완벽한 정보가 필요

한계를 극복하기위해 환경을 모르지만 환경과 상호작용을 통해 경험을 바탕으로 학습하는 방법이 등장

(모델 없이 학습하는 강화학습)



MDP에서 환경의 모델 - 상태 변환 확률과 보상

모델링 - 입력과 출력의 관계를 식으로 나타내는 과정

- 할 수 있는 선에서 정확한 모델링을 한 후, 모델링에 오차부분을 실험을 통해 조정
- 모델 없이 환경과 상호작용을 통해 입력 출력 사이의 관계를 학습시킴



#### policy iteration

- 현재 정책에 대한 참 가지함수를 구하는 `정책 평가`와 평가한 내용을 가지고 정책을 업데이트하는 `정책 발전`으로 이루어짐, `벨만 기대 방정식`을 이용,구한 가치를 토대로 최대의 보상을 얻게하는 행동을 선택하는 `탐욕 정책발전`을 이용

#### value iteration

- 최적 정책을 가정, `벨만 최적 방정식`을 이용해 순차적 행동 결정 문제에 접근, 정책이 직접적으로 주어지지 않음, 행동의 선택은 가치함수를 통해 이뤄짐